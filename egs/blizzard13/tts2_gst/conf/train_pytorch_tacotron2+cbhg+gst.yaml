# This is tactron2 + CBHG + GST training settting
# model
model-module: espnet.nets.pytorch_backend.e2e_tts_tacotron_gst:Tacotron2_GST

# encoder related
embed-dim: 512
elayers: 1
eunits: 512
econv-layers: 3 # if set 0, no conv layer is used
econv-chans: 512
econv-filts: 5

# decoder related
dlayers: 2
dunits: 512
prenet-layers: 2  # if set 0, no prenet is used
prenet-units: 256
postnet-layers: 5 # if set 0, no postnet is used
postnet-chans: 512
postnet-filts: 5

# attention related
atype: forward_ta
adim: 128
aconv-chans: 32
aconv-filts: 15      # resulting in filter-size = aconv-filts * 2 + 1
cumulate-att-w: true # whether to cumulate attetion weight
use-batch-norm: true # whether to use batch normalization in conv layer
use-concate: true    # whether to concatenate encoder embedding with decoder lstm outputs
use-residual: false  # whether to use residual connection in encoder convolution
use-masking: true    # whether to mask the padded part in loss calculation
bce-pos-weight: 1.0  # weight for positive samples of stop token in cross-entropy calculation
reduction-factor: 1

# cbhg related (mels -> mags)
use-cbhg: true
use-second-target: true
cbhg-conv-bank-layers: 8
cbhg-conv-bank-chans: 128
cbhg-conv-proj-filts: 3
cbhg-conv-proj-chans: 256
cbhg-highway-layers: 4
cbhg-highway-units: 128
cbhg-gru-units: 256

# minibatch related
batch-size: 32
sortagrad: 0       # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs
batch-sort-key: output # shuffle(dynamic batchsize deactivated) or input or output
#maxlen-in: 300     # if input length  > maxlen-in, batchsize is reduced (if use "shuffle", not effect)
#maxlen-out: 800    # if output length > maxlen-out, batchsize is reduced (if use "shuffle", not effect)
batch-frames-out: 1200 # "Maximum input frames in a minibatch (0 to disable)"
batch-frames-in: 0    # Maximum input frames in a minibatch (0 to disable)"


# optimization related
lr: 1e-3
eps: 1e-6
weight-decay: 0.0
dropout-rate: 0.5
zoneout-rate: 0.1
epochs: 200
patience: 20

# reference related
ref-embed-dim: 512
style-embed-dim: 512